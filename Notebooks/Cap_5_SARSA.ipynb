{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <h1>\n",
    "        SARSA\n",
    "    </h1>\n",
    "</div>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    Nesse Notebook vamos implementar um metodo on-policy que aprende por tentativa e erro e utiliza bootstrapping.\n",
    "    O nome SARSA se da por utilizar a seguinte regra de atualizacao\n",
    "</div>\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{State}_t, \\text{Action}_t, \\text{Reward}_t, \\text{State}_{t+1}, \\text{Action}_{t+1}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar os pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from envs import Maze\n",
    "from utils import plot_policy, plot_action_values, test_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar o ambiente, tabela de valores e politica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criar o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criar a tabela de valores $Q(s, a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_values = np.zeros(shape=(5, 5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criar a politica $\\pi(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, epsilon=0.):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    else:\n",
    "        av = action_values[state]\n",
    "        return np.random.choice(np.flatnonzero(av == av.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotar a tabela de valores $Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_action_values(action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotar a politica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_policy(action_values, env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementar o algoritmo\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    Adapted from Barto & Sutton: \"Reinforcement Learning: An Introduction\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(action_values, policy, episodes, alpha=0.1, gamma=0.99, epsilon=0.2):\n",
    "    \n",
    "    for episode in range(1, episodes + 1):\n",
    "        state = env.reset()\n",
    "        action = policy(state, epsilon)\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_action = policy(next_state, epsilon)\n",
    "            \n",
    "            qsa = action_values[state][action]\n",
    "            next_qsa = action_values[next_state][next_action]\n",
    "            action_values[state][action] = qsa + alpha * (reward + gamma * next_qsa - qsa)\n",
    "            state = next_state\n",
    "            action = next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sarsa(action_values, policy, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrar os resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mostrar a tabela resultante $Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_action_values(action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mostrar a politica resultante $\\pi(\\cdot|s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(action_values, env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testar o agente resultante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent(env, policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('mltools')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "055a87f69c3ebf47a00d23f13067ddfe7185589aa8787ffbe77b4752a8593c1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
