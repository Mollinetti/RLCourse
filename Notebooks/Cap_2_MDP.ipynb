{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Processo de Decisao de Markov\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Nesse Notebook, vamos introduzir o processo de decisao de Markov, vamos utilizar a biblioteca gym do OpenAI para isso\n",
    "<br><br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from envs import Maze\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construindo o Ambiente: Maze()\n",
    "\n",
    "Esse metodo retorna uma instance da classe Maze. Nao precisaremos mexer por enquanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### env.reset()\n",
    "\n",
    "Reiniciar o ambiente de labirinto para que possamos observar o agente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = env.reset()\n",
    "print(f\"O novo episodio vai iniciar no estado: {initial_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### env.render()\n",
    "\n",
    "Gera a imagem do atual estado que se encontra o labirinto em formato np.ndarray. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame = env.render(mode='rgb_array')\n",
    "plt.axis('off')\n",
    "plt.title(f\"State: {initial_state}\")\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### env.step()\n",
    "\n",
    "Esse metodo aplica a acao selecionada pelo agente no ambiente a fim de modifica-lo. Em resposta, o ambiente retorna uma tupla de quatro objetos\n",
    "\n",
    "- O proximo estado\n",
    "- O pagamento obtido\n",
    "- Se a tarefa foi completa (T/F)\n",
    "- Se ha qualquer outra informacao relevante em forma de dicionario \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 2\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(f\"Após mover 1 linha para baixo, o agente esta no estado: {next_state}\")\n",
    "print(f\"Após mover 1 linha para baixo, temos um pagamento de: {reward}\")\n",
    "print(\"Após mover 1 linha para baixo, a tarefa \", \"\" if done else \"não \", \"acabou\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Renderizar o novo estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = env.render(mode='rgb_array')\n",
    "plt.axis('off')\n",
    "plt.title(f\"State: {next_state}\")\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### env.close()\n",
    "\n",
    "Finalizar a tarefa e fechar o ambiente, liberando os recursos computacionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labirinto: encontre a saída.\n",
    "\n",
    "Vamos utilizar esse ambiente para os proximos algoritmos: PD, Monte-Carlo e TD. Esse ambiente é dito apropriado para aprender o basico de RL pois:\n",
    "\n",
    "- Possui poucos estados (25)\n",
    "- Transicoes entre estados sao deterministicos ($p(s', r| s, a) = 1$)\n",
    "- Todos os pagamentos sao os mesmos (-1) ate que o episodio conclua. Isso facilita o estudo de funcoes de $V(S)$ e $Q(S,A)$\n",
    "\n",
    "Atraves desse ambiente, vamos revisar os conceitos vistos anteriormente:\n",
    "\n",
    "- Estados\n",
    "- Acoes\n",
    "- Trajetorias\n",
    "- Retornos e pagamentos\n",
    "- Politica\n",
    "\n",
    "O ambiente é um labirinto 5x5 no qual a meta do agente é encontrar a saída do labirinto, localizada no canto direito de baixo, na celula (4,4), colorida de verde claro.\n",
    "\n",
    "Para chegar a saida, o agente pode tomar quatro diferente acoes: mover para cima, baixo, direita e esquerda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Criar o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Espaço dos Estados\n",
    "\n",
    "Os estados consistem em uma tuple com dois numeros inteiros no intervalo $[0,4]$ representando a linha e a coluna em que o agente está atualmente localizado:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "    s = (row, column) \\;\\\\\n",
    "    row, column \\in \\{0,1,2,3, 4\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "<br>\n",
    "O espaço de busca possui 25 elementos (todas a possiveis combinacoes de linhas e colunas)\n",
    "\n",
    "\\begin{equation}\n",
    "    Rows \\times Columns \\;\\\\\n",
    "    S = \\{(0, 0), (0, 1), (1, 0), ...\\}\n",
    "\\end{equation}\n",
    "\n",
    "Informacao sobre o espaço de busca está armazenado na variavel env.observation_space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"O estado inicial é: {env.reset()}\")\n",
    "print(f\"O espaço de busca é do tipo: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ações e espaço de ações\n",
    "\n",
    "Neste ambiente, existem quatro diferentes ações que sāo reprensentados por numeros inteiros:\n",
    "\n",
    "\\begin{equation}\n",
    "a \\in \\{0, 1, 2, 3\\}\n",
    "\\end{equation}\n",
    "\n",
    "- 0 -> mover para cima\n",
    "- 1 -> mover para direita\n",
    "- 2 -> mover para baixo\n",
    "- 3 -> mover para esquerda\n",
    "\n",
    "Para executar uma acao, passe como um argumento para o metodo env.step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Um exemplo de ação valida é: {env.action_space.sample()}\")\n",
    "print(f\"O espaço de ação é do tipo: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trajetorias e episodios\n",
    "\n",
    "Uma Trajetoria é a sequencia gerada quando se move de um estado arbitrario para o outro\n",
    "\n",
    "\\begin{equation}\n",
    "  \\tau = S_0, A_0, R_1, S_1, A_1, ... R_N, S_N,\n",
    "\\end{equation}\n",
    "\n",
    "Vamos gerar uma trajetoria de tres estados abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "trajectory = []\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    trajectory.append([state, action, reward, done, next_state])\n",
    "    state = next_state\n",
    "env.close()\n",
    "\n",
    "print(f\"Você gerou a sua primeira trajetoria:\\n{trajectory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um episodio é a trajetoria que vai do estado inicial do processo até o final \n",
    "\n",
    "\\begin{equation}\n",
    "  \\tau = S_0, A_0, R_1, S_1, A_1, ... R_T, S_T,\n",
    "\\end{equation}\n",
    "Aonde T é o estado terminal\n",
    "\n",
    "Vamos gerar um episodio completo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "episode = []\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    episode.append([state, action, reward, done, next_state])\n",
    "    state = next_state\n",
    "env.close()\n",
    "\n",
    "print(f\"Você gerou o seu primeiro episodio:\\n{episode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pagamentos e retornos\n",
    "\n",
    "Um pagamento é o feedback numero que o ambiente gera quando o agente toma uma ação *a* num estado *s*:\n",
    "\n",
    "\\begin{equation}\n",
    "    r = r(s, a)\n",
    "\\end{equation}\n",
    "\n",
    "Vamos gerar um pagamento atraves do ambiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "_, reward, _, _ = env.step(action)\n",
    "print(f\"Obteve-se um pagamento de {reward} tomando a ação {action} no estado {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O retorno associado com o momento no tempo *t* é a soma (descontada) dos pagamentos que o agente obtém daquele momento.\n",
    "Vamos calcular $G_0$, o retorno no inicio do episodio:\n",
    "\n",
    "\\begin{equation}\n",
    "    G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... + \\gamma^{T-1} R_T\n",
    "\\end{equation}\n",
    "\n",
    "Assumindo que o fator de desconto $\\gamma = 0.99$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "done = False\n",
    "gamma = 0.99\n",
    "G_0 = 0\n",
    "t = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, done, _ = env.step(action)\n",
    "    G_0 += gamma ** t * reward\n",
    "    t += 1\n",
    "env.close()\n",
    "\n",
    "print(\n",
    "    f\"\"\"Tomou-se {t} movimentos para encontrar a saída, \n",
    "    e cada pagamento r(s,a)=-1, entao o retorno se equivale a {G_0}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Politica\n",
    "\n",
    "Uma politica é uma função $\\pi(a|s) \\in [0, 1]$ que dá a probabilidade de uma ação dado um estado. A função possui como entrada um estado e uma ação e retorna um numero de ponto fluante entre $[0,1]$\n",
    "\n",
    "já que na pratica vamos precisar computar as probabilidades de todas as acoes, vamos representar a politica como uma funcao que toma um estado como argumento e retorna as probabilidades associadas com cada acao, um exemplo seria:\n",
    "\n",
    "[0.5, 0.3, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return np.array([0.25] * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simular um episodio utilizando a politica aleatoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Criar e reiniciar o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computar $p(a|s) \\; \\forall a \\in \\{0, 1, 2, 3\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probabilities = random_policy(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = ('Up', 'Right', 'Down', 'Left')\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "plt.bar(y_pos, action_probabilities, alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('P(a|s)')\n",
    "plt.title('Politica Estocastica')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usar a politica para simular um episodio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "img = plt.imshow(env.render(mode='rgb_array')) \n",
    "while not done:\n",
    "    action = np.random.choice(range(4), 1, p=action_probabilities)\n",
    "    _, _, done, _ = env.step(action)\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('mltools')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "055a87f69c3ebf47a00d23f13067ddfe7185589aa8787ffbe77b4752a8593c1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
